{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLfsr38dvF0lHJHqLvK5rZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickFury89/Car-race-game/blob/master/%D0%A1%D0%B5%D1%80%D0%B2%D0%B8%D1%81_%D0%BC%D0%BE%D0%BD%D0%B8%D1%82%D0%BE%D1%80%D0%B0_%D0%BA%D0%BD%D0%B8%D0%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bеб-скрейпер на Python для сбора метаданных и текстов книг из онлайн-магазинов и библиотек:**"
      ],
      "metadata": {
        "id": "r_TC_MjcFgcu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdB9Nb1k_nL6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_book(url):\n",
        "    \n",
        "    # Отправляем GET-запрос на страницу с книгой\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    \n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Извлекаем метаданные книги\n",
        "    \n",
        "    title = soup.find('h1', {'class': 'book-title'}).text.strip()\n",
        "    \n",
        "    author = soup.find('div', {'class': 'book-author'}).find('a').text.strip()\n",
        "    \n",
        "    genre = soup.find('span', {'class': 'genre'}).find('a').text.strip()\n",
        "    \n",
        "    description = soup.find('div', {'class': 'book-annotation'}).text.strip()\n",
        "    \n",
        "    cover_url = soup.find('img', {'class': 'book-image'})['src']\n",
        "\n",
        "    # Извлекаем текст книги\n",
        "    \n",
        "    book_text = ''\n",
        "    \n",
        "    for page_num in range(1, 101):\n",
        "        \n",
        "        page_url = f'{url}/chapter-{page_num}'\n",
        "        \n",
        "        response = requests.get(page_url)\n",
        "        \n",
        "        page_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        \n",
        "        page_text = page_soup.find('div', {'class': 'reader-container'}).text.strip()\n",
        "        \n",
        "        book_text += page_text + '\\n'\n",
        "\n",
        "    # Возвращаем словарь с метаданными и текстом книги\n",
        "    \n",
        "    return {\n",
        "        \n",
        "        'title': title,\n",
        "        \n",
        "        'author': author,\n",
        "        \n",
        "        'genre': genre,\n",
        "        \n",
        "        'description': description,\n",
        "        \n",
        "        'cover_url': cover_url,\n",
        "        \n",
        "        'book_text': book_text,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Код обработки естественного языка для извлечения функций библиотеки из текста книг"
      ],
      "metadata": {
        "id": "4c3SKkAtFsiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def extract_library_functions(book_text):\n",
        "    \n",
        "    # Разбиваем текст книги на предложения и токены\n",
        "    \n",
        "    sentences = sent_tokenize(book_text)\n",
        "    \n",
        "    tokens = word_tokenize(book_text.lower())\n",
        "\n",
        "    # Определяем список стоп-слов и лемматизатор\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Извлекаем глаголы, прилагательные и существительные\n",
        "    \n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    \n",
        "    verbs = [lemmatizer.lemmatize(word) for word, tag in pos_tags if tag.startswith('VB')]\n",
        "    \n",
        "    adjectives = [lemmatizer.lemmatize(word) for word, tag in pos_tags if tag.startswith('JJ')]\n",
        "    \n",
        "    nouns = [lemmatizer.lemmatize(word) for word, tag in pos_tags if tag.startswith('NN')]\n",
        "\n",
        "    # Отфильтровываем стоп-слова и возвращаем уникальные значения\n",
        "    \n",
        "    verbs = list(set(verbs) - stop_words)\n",
        "    \n",
        "    adjectives = list(set(adjectives) - stop_words)\n",
        "    \n",
        "    nouns = list(set(nouns) - stop_words)\n",
        "\n",
        "    # Возвращаем словарь с глаголами, прилагательными и существительными\n",
        "    \n",
        "    return {\n",
        "        \n",
        "        'verbs': verbs,\n",
        "        \n",
        "        'adjectives': adjectives,\n",
        "        \n",
        "        'nouns': nouns,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "XzAw_CJxAbHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Машинное обучение"
      ],
      "metadata": {
        "id": "fTuU6lAEBNNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import joblib\n",
        "import sqlite3\n",
        "\n",
        "# Подключаемся к базе данных и создаем таблицу для результатов\n",
        "conn = sqlite3.connect('books.db')\n",
        "c = conn.cursor()\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS model_results \n",
        "             (model_name text, accuracy real)''')\n",
        "\n",
        "# Загружаем данные и разделяем их на обучающую и тестовую выборки\n",
        "data = pd.read_csv('books_data.csv')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['genre'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучаем модели на обучающей выборке\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Оцениваем точность моделей на тестовой выборке\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Сохраняем модели и результаты в базу данных\n",
        "    joblib.dump(model, f'{name}.joblib')\n",
        "    c.execute(\"INSERT INTO model_results VALUES (?, ?)\", (name, accuracy))\n",
        "\n",
        "# Сохраняем изменения в базе данных и закрываем соединение\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "SCvk1ZEBCOC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# База данных"
      ],
      "metadata": {
        "id": "TRIqr_pmDO--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE books (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    title TEXT,\n",
        "    author TEXT,\n",
        "    genre TEXT,\n",
        "    language TEXT,\n",
        "    publication_date TEXT,\n",
        "    publisher TEXT,\n",
        "    text TEXT,\n",
        "    features TEXT,\n",
        "    analysis_result TEXT\n",
        ");\n"
      ],
      "metadata": {
        "id": "HzEhgAaRDRFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Чтобы сохранить данные в базе данных, мы можем использовать трафик sqlite3в Python. Вот пример кода, который добавляет данные в таблицу books:"
      ],
      "metadata": {
        "id": "jhJWF_TPDZU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('books.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Добавляем данные в таблицу\n",
        "cursor.execute(\"INSERT INTO books (title, author, genre, language, publication_date, publisher, text, features, analysis_result) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\", (title, author, genre, language, publication_date, publisher, text, features, analysis_result))\n",
        "conn.commit()\n",
        "\n",
        "# Закрываем соединение с базой данных\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "GYMZi34EDd9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для создания внешнего вида для поиска книг по созданию параметров и страниц для просмотра информации о книгах и их оценках можно использовать веб-фреймворк, например Flask. Ниже представлен пример кода для создания простого веб-приложения с возможностью поиска книг по жанру и просмотра информации о книгах:\n"
      ],
      "metadata": {
        "id": "NgJNh1aXD6WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request\n",
        "import sqlite3\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('home.html')\n",
        "\n",
        "@app.route('/search', methods=['GET', 'POST'])\n",
        "def search():\n",
        "    if request.method == 'POST':\n",
        "        genre = request.form['genre']\n",
        "        conn = sqlite3.connect('books.db')\n",
        "        c = conn.cursor()\n",
        "        c.execute(\"SELECT * FROM books WHERE genre=?\", (genre,))\n",
        "        books = c.fetchall()\n",
        "        conn.close()\n",
        "        return render_template('search_results.html', books=books)\n",
        "    else:\n",
        "        return render_template('search.html')\n",
        "\n",
        "@app.route('/book/<int:book_id>')\n",
        "def book(book_id):\n",
        "    conn = sqlite3.connect('books.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT * FROM books WHERE id=?\", (book_id,))\n",
        "    book = c.fetchone()\n",
        "    c.execute(\"SELECT * FROM ratings WHERE book_id=?\", (book_id,))\n",
        "    ratings = c.fetchall()\n",
        "    conn.close()\n",
        "    return render_template('book.html', book=book, ratings=ratings)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "UbcoonPhEBPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Развернуть систему на облачном сервисе для обеспечения масштабируемости и отказоустойчивости. используйте облачные сервисы для хранения данных и файлы с шаблонами машинного обучения. Настроены высокие масштабы и мониторинг состояния системы."
      ],
      "metadata": {
        "id": "7DBB80EhFRbk"
      }
    }
  ]
}